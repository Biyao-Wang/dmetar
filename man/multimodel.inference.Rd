% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/mreg.multimodel.inference.R
\name{multimodel.inference}
\alias{multimodel.inference}
\title{Perform multimodel inference on a meta-regression model}
\usage{
multimodel.inference(TE, seTE, data, predictors, method='REML', test='knha',
    eval.criterion='AICc', interaction=FALSE, seed = 123)
}
\arguments{
\item{TE}{The precalculated effect size for each study. Must be supplied as the name of the effect size
column in the dataset (in quotation marks; e.g. \code{TE = "effectsize"}).}

\item{seTE}{The precalculated standard error for each study. Must be supplied as the name of the standard error
column in the dataset (in quotation marks; e.g. \code{seTE = "se"}).}

\item{data}{A \code{data.frame} containing columns for the effect size, standard error and
meta-regression predictors of each study/effect.}

\item{predictors}{A concentenated array of characters specifying the predictors to be used
for multimodel inference. Names of the predictors must be identical to the names of the column
names of the \code{data.frame} supplied to \code{data}.}

\item{method}{Meta-analysis model to use for pooling effect sizes. Use \code{'FE'} for the
fixed-effect model. Different random-effect models are available: \code{'DL', 'HE', 'SJ', 'ML', 'REML', 'EB', 'HS', 'GENQ'}.
If \code{'FE'} is used, the \code{test} argument is automatically set to \code{'z'}, as the Knapp-Hartung
method is not meant to be used with fixed-effect models. Default is \code{'REML'}, and it is strongly advised to remain with
this option to use a standard (mixed-effects) meta-regression model.}

\item{test}{Method to use to compute test statistics and confidence intervals. Default is \code{'knha'}
which uses the Knapp-Hartung (Knapp & Hartung, 2003) adjustment method. "Conventional" Wald-type tests and
CIs are calculated by setting this argument to \code{'z'}. When \code{method='FE'}, this argument is
set to \code{'z'} automatically as the Knapp-Hartung method was not meant to be used with fixed-effect models.}

\item{eval.criterion}{Evaluation criterion to sort the multiple models by. Can be either \code{'AICc'}
(default; corrected Akaike's Information Criterion), \code{'AIC'} (Akaike's Information Criterion) or
\code{'BIC'} (Bayesian Information Criterion).}

\item{interaction}{If set to \code{FALSE} (default), no interactions between predictors are considered. Setting this parameter to
\code{TRUE} means that all interactions are modeled.}

\item{seed}{Optional. Set a seed for the function.}
}
\value{
Returns four tables and a plot:
\itemize{
\item \strong{Final Results (Summary Table)}: Displays the number of fitted models, model formula,
method to calculate test statistics and confidence intervals, interactions, and evaluation criterion used.
\item \strong{Best 5 Models}: Displays the top five models in terms of the evaluation criterion used.
Predictors are displayed as columns of the table, and models as rows. A number (weight) or \code{+}
sign (for categorical predictors) indicates that a predictor/interaction term was used for the
model, while empty cells indicate that the predictor was omitted in this model. Other metrics such as the
\code{weight}, evaluation metric \code{delta} compared to the best model, logLikelihood and degrees of freedom
are also diplayed
\item \strong{Multimodel Inference Coefficients}: Displays the coefficients and statistical significance
of each regression term in the model.
\item \strong{Predictor Importance}: Displays the importance value for each model term. The table is sorted from
highest to lowest. A common rule of thumb is to consider a predictor as important when its importance value is above 0.8.
\item \strong{Predictor Importance Plot}: A bar plot for the predictor importance data along with a reference line for the
0.8 value often used as a crude threshold to characterize a predictor as important.
}
}
\description{
This function performs multimodel inference to evaluate the importance of predictors
in a meta-analytical meta-regression model.
}
\details{
Multi-model methods differ from stepwise methods as they do not try to successively build
the “best” one (meta-regression) model explaining most of the variance. Instead, in this procedure,
all possible combinations of a predifined selection of predictors are modeled, and evaluated using
a criterion such as Akaike’s Information Criterion, which rewards simpler models.
This enables a full eximination of all possible models, and how they perform.
A common finding using this procedure is that there are many different kinds of predictor
combinations within a model which lead to a good fit. In multimodel inference, the estimated
coefficients of predictors can then be synthesized across all possible models to infer how
important certain predictors are overall.

Multimodel Inference can be a useful way to obtain a comprehensive look on which predictors are
more or less important for predicting differences in effect sizes. Despite avoiding some of the
problems of stepwise regression methods, it should be noted that this method should still be rather
seen as exploratory, and may be used when there are no prior knowledge on how our predictors are
related to effect sizes in the research field under study.

The \code{multimodel.inference} function calls the \code{\link[metafor]{rma.uni}} function internally,
which is then fed forward to the \code{\link[MuMIn]{dredge}} function for multimodel inference through
two utility functions returned by the \code{multimodel.inference} function.

Parts of the computations in this function are based on a vignette by Wolfgang Viechtbauer, which can be found
\href{http://www.metafor-project.org/doku.php/tips:model_selection_with_glmulti_and_mumin}{here}.
}
\examples{
\dontrun{
# Example 1: Perform multimodel inference with default settings
data('MVRegressionData')
library(metafor)
mmi = multimodel.inference(TE = 'yi', seTE = 'sei', data = MVRegressionData,
                           predictors = c('pubyear', 'quality',
                                          'reputation', 'continent'))

# Example 2: Model Interaction terms, set method to 'DL',
# change evaluation criterion to bic
multimodel.inference(TE = 'yi', seTE = 'sei', data = MVRegressionData,
                     predictors = c('pubyear', 'quality',
                                    'reputation', 'continent'),
                     method='DL', eval.criterion = 'BIC', interaction = TRUE)

# Example 3: Use only categorical predictors
data('ThirdWave')
multimodel.inference(TE = 'TE', seTE = 'seTE', data = ThirdWave,
                     predictors = colnames(ThirdWave)[4:7], interaction = FALSE)}
}
\references{
Harrer, M., Cuijpers, P., Furukawa, T.A, & Ebert, D. D. (2019).
\emph{Doing Meta-Analysis in R: A Hands-on Guide}. DOI: 10.5281/zenodo.2551803. \href{https://bookdown.org/MathiasHarrer/Doing_Meta_Analysis_in_R/smallstudyeffects.html}{Chapter 9.1}

Knapp, G., & Hartung, J. (2003). Improved tests for a random effects meta-regression with a single covariate.
\emph{Statistics in Medicine, 22}, 2693–2710.

Viechtbauer, W. (2019). \emph{Model Selection using the glmulti and MuMIn Packages}. \href{http://www.metafor-project.org/doku.php/tips:model_selection_with_glmulti_and_mumin}{Link}.
Last accessed 01-Aug-2019.
}
\seealso{
\code{\link[MuMIn]{dredge}}
}
\author{
Mathias Harrer & David Daniel Ebert
}
